{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d9b3ae",
   "metadata": {},
   "source": [
    "Pandas, to open data files and to apply certain operations to the data.\n",
    "Html, to decode HTML entities into regular characters.\n",
    "Re, to filter and delete unnecessary links, hash, username, punctuations or whatever you wish.\n",
    "Nltk, to clean stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7832a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a72b4",
   "metadata": {},
   "source": [
    "we need to import the Twitter data. in this case, I use CSV Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a67a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‘' (U+2018) (Temp/ipykernel_12528/325302982.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/325302982.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pd.set_option(‘display.max_colwidth’, None)\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '‘' (U+2018)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(‘display.max_colwidth’, None)\n",
    "data = pd.read_csv(‘your_sample.csv’)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741e2f0",
   "metadata": {},
   "source": [
    "Once we have imported the data, we’re now ready for the data cleaning process. The first things that we’re going to clean are data duplicates. Most of the time, we don’t need the data duplicates, because in further use (i.e. analysis) these data duplicates could mess up the result by messing up the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32d295b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‘' (U+2018) (Temp/ipykernel_12528/1074731155.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/1074731155.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    new_data = data.drop_duplicates(‘Tweet Content’,keep=’first’) #delete the duplicates by dropping them and store the result value to a new variable\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '‘' (U+2018)\n"
     ]
    }
   ],
   "source": [
    "new_data = data.drop_duplicates(‘Tweet Content’,keep=’first’) #delete the duplicates by dropping them and store the result value to a new variable\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97441a5",
   "metadata": {},
   "source": [
    "If your dataframe have indices included on it, once you drop those data duplicates, you need to store the new dataframe in a new file. Don’t forget to store the new dataframe to a new file without including the index on it, so that we could explore the data more freely later on.\n",
    "\n",
    "We’re here assuming that we’re only going to use the tweets data, so we’re going to extract the tweets data out of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d8cf7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '’' (U+2019) (Temp/ipykernel_12528/4065702345.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/4065702345.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    new_data.to_csv(r’your_new_sample.csv’, index = False)\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '’' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "new_data.to_csv(r’your_new_sample.csv’, index = False)\n",
    "new_sample = pd.read_csv(‘your_new_sample.csv’)\n",
    "new_sample.head()\n",
    "tweets = new_sample[‘Tweet Content’]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad07c9",
   "metadata": {},
   "source": [
    "Once we extracted tweet data, we’ll notice things that need to be cleaned. Most of the time, the tweets returned by Twitter JSON data contain HTML entities and they need to be decoded into characters. So, we’re cleaning them using html library. Apart from that, we also need to clean up newlines since they make the data messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b83869",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_12528/3699276296.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/3699276296.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    x = tweets[i].replace(“\\n”,” “) #cleaning newline “\\n” from the tweets\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(tweets)):\n",
    "x = tweets[i].replace(“\\n”,” “) #cleaning newline “\\n” from the tweets\n",
    "tweets[i] = html.unescape(x)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3b142",
   "metadata": {},
   "source": [
    "Sometimes when tweeting, Twitter users attach media like pictures, videos, etc. Those media will be converted into links on the JSON data. Since we’re only going to be using the text data, which is the tweets, so we need to clean up the links. Also, we will clean up hash characters (only the hash characters not the whole hashtags) and username. All those things will be cleaned using the regex Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0246b30e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_12528/1519510296.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/1519510296.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tweets[i] = re.sub(r”(@[A-Za-z0–9_]+)|[^\\w\\s]|#|http\\S+”, “”, tweets[i])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(tweets)):\n",
    "tweets[i] = re.sub(r”(@[A-Za-z0–9_]+)|[^\\w\\s]|#|http\\S+”, “”, tweets[i])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117fc5c",
   "metadata": {},
   "source": [
    "Up till now, we already got much cleaner data, but there is one more thing that we need to do to make it even cleaner. In text-data, mostly it contains insignificant words that are not used for the analysis process because they could mess up the analysis score. So, we’re about to clean them now using the nltk Python library. There are several steps you need to do to remove the stopwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a3bb2",
   "metadata": {},
   "source": [
    "- Preparing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cf1eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‘' (U+2018) (Temp/ipykernel_12528/2672598555.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/2672598555.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    sw = stopwords.words(‘english’) #you can adjust the language as you desire\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '‘' (U+2018)\n"
     ]
    }
   ],
   "source": [
    "tweets_to_token = tweets\n",
    "sw = stopwords.words(‘english’) #you can adjust the language as you desire\n",
    "sw.remove(‘not’) #we exclude not from the stopwords corpus since removing not from the text will change the context of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99002c50",
   "metadata": {},
   "source": [
    "- Tokenize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef87ec85",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_12528/366337253.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/366337253.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tweets_to_token[i] = word_tokenize(tweets_to_token[i])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tweets_to_token)):\n",
    "tweets_to_token[i] = word_tokenize(tweets_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4dbe3",
   "metadata": {},
   "source": [
    "- Remove the Stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdbaac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_12528/3009147051.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\NOURED~1\\AppData\\Local\\Temp/ipykernel_12528/3009147051.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tweets_to_token[i] = [word for word in tweets_to_token[i] if not word in sw]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tweets_to_token)):\n",
    "tweets_to_token[i] = [word for word in tweets_to_token[i] if not word in sw]\n",
    "tweets_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fdf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
